"""
Script to calculate ROUGE and BLEU scores for summaries generated by models on MEDIQA-AnS

To evaluate all models:
python summarization_evaluation.py --dataset=chiqa --bleu --evaluate-models
Evaluate all models as well as save excel sheet comparing rouge per summ b/w model and ref (task/summ type hardcoded).
python summarization_evaluation.py --dataset=chiqa --bleu --evaluate-models --rouge-per-sample
There is also a separate --bleu-per-sample option
python summarization_evaluation.py --dataset=chiqa --bleu --evaluate-models --bleu-per-sample
Or evaluate all models but compute Wilcoxon between BART and the Pointer-Generator while you're at it.
python summarization_evaluation.py --dataset=chiqa --bleu --evaluate-models --wilcoxon
--bleu is always optional. Script computes ROUGE metrics by default.

Question-Driven tests
Question-driven BART comparisons
python summarization_evaluation.py --dataset=chiqa --bleu --q-driven
Q Driven with rouge per sample to excel sheet (model/task/summ type hardcoded):
python summarization_evaluation.py --dataset=chiqa --bleu --q-driven --rouge-per-sample
Or compute wilcoxon b/w BART models with and without question concatenated to input text:
python summarization_evaluation.py --dataset=chiqa --q-driven --wilcoxon

"""


import rouge
import argparse
import pandas as pd
import sys
import json
from collections import Counter
import nltk
import numpy as np
import scipy


def get_args():
    """
    Argument defnitions
    """
    parser = argparse.ArgumentParser(description="Arguments for data exploration")
    parser.add_argument("--rouge-per-sample",
                        dest="rouge_per_sample",
                        action="store_true",
                        help="Calulate rouge scores for each sentence and save the examples to file")
    parser.add_argument("--bleu",
                        dest="calculate_bleu",
                        action="store_true",
                        help="Calulate bleu for each system. By default the script reports just rouge")
    parser.add_argument("--bleu-per-sample",
                        dest="bleu_per_sample",
                        action="store_true",
                        help="Calulate bleu scores for each sentence and save the examples to file. Can be run without --bleu")
    parser.add_argument("--dataset",
                        dest="dataset",
                        help="Dataset to run baselines on. Currently only configured for CHiQA.")
    parser.add_argument("--wilcoxon",
                        dest="calculate_wilcoxon",
                        action="store_true",
                        help="Compute wilcoxn statistics between rouge scores of models")
    parser.add_argument("--q-driven",
                        dest="evaluate_question_driven",
                        action="store_true",
                        help="Compare pg and BART question driven and non-question driven models")
    parser.add_argument("--evaluate-models",
                        dest="evaluate_all_models",
                        action="store_true",
                        help="Evaluate all the models and baselines")
    return parser


class DataProcessor():
    """
    Class for pointer generator evaluation
    """

    def load_data(self, data_path):
        """
        Load specified dataset and process for evaluation
        """
        with open(data_path, "r", encoding="utf-8") as f:
            eval_data = json.load(f)

        ref_summaries, gen_summaries, questions = self.process_data(eval_data)
        return ref_summaries, gen_summaries, questions

    def process_data(self, data):
        """
        Get the gold summaries and generated summaries
        """
        ref_summaries = []
        gen_summaries = []
        questions = []
        for gold, gen, q in zip(data['ref_summary'], data['gen_summary'], data['question']):
            ref_summaries.append(gold)
            gen_summaries.append(gen)
            questions.append(q)
        return ref_summaries, gen_summaries, questions


def tokenize(summaries):
    """
    For each summary and generated summary, tokenize on spaces
    """
    tokenized_summaries = [nltk.word_tokenize(summ) for summ in summaries]
    return tokenized_summaries


def compute_nltk_bleu(eval_name, eval_set):
    """
    Use NLTK to compute bleu score for up to 4 ngrams
    """
    guess = eval_set[1]
    gold = eval_set[0]
    # Each ref has to be a list of a list, as nltk likes to be able to count for multiple sentences per doc
    gold = [[g] for g in gold]
    bleu_score = nltk.translate.bleu_score.corpus_bleu(gold, guess)
    return bleu_score


def words_to_ngrams(words, n=2):
    ngrams = ["_".join(t) for t in zip(*(words[i:] for i in range(n)))]
    return ngrams


def compute_fast_rouge(guess_summs, gold_summs, n=2):
    summ_cnt = 0
    macro_rouge_list = []
    for guess, gold in zip(guess_summs, gold_summs):
        summ_cnt += 1
        guess = words_to_ngrams(guess, n)
        gold = words_to_ngrams(gold, n)
        f_guess = Counter(guess)
        f_gold = Counter(gold)
        macro_rouge_sum = 0
        for ngram in f_gold:
            macro_rouge_sum += min(f_guess[ngram], f_gold[ngram])
        macro_rouge_list.append(np.float32(macro_rouge_sum) / np.float32(len(gold)))

    macro_rouge = sum(macro_rouge_list) / summ_cnt
    return macro_rouge


def calculate_rouge(eval_name, eval_set, summ_task, summ_type):
    """
    Evaluate generated summaries with py-rouge
    Expects two lists, one of reference summaries and the other
    of generated summaries where the indexes correspond.
    """
    evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l'],
                          max_n=3,
                          limit_length=False,
                          #length_limit=100,
                          length_limit_type='words',
                          apply_avg=True,
                          apply_best=False,
                          alpha=1,
                          weight_factor=1.2,
                          stemming=False)

    # eval_set[1] == gen_summaries
    # eval_set[0] == ref_summaries
    metrics = []
    rouge_scores = evaluator.get_scores(eval_set[1], eval_set[0])
    results_file.write("\nPerformance by {e} on {l}, {t}\n".format(e=eval_name, l=summ_task, t=summ_type))
    for metric, results in sorted(rouge_scores.items(), key=lambda x: x[0]):
        if metric == "rouge-l":
            result = results['r']
        else:
            result = results['r']
        metrics.append(result)
        results_file.write("{m}\t{r}\n".format(m=metric, r=result))
    return metrics


def evaluate():
    """
    Main function for evaluating summaries
    """
    if args.evaluate_all_models:
        # Wilcoxon for BART v PG
        question_string = "without_question"
        if args.calculate_wilcoxon:
            rouge_dict = {}
        for summ_task in ['page2answer', 'section2answer']:
            for summ_type in ['single_abstractive', 'single_extractive']:
                print("evaluating: {l}, {t}".format(l=summ_task, t=summ_type))
                if summ_task == "page2answer":
                    baseline_summ_task = "p2a"
                else:
                    baseline_summ_task = "s2a"
                # Currently just doing Lead-3 baseline.
                if summ_type == "single_abstractive":
                    baseline_summ_type = "single-abs"
                    k = 3
                else:
                    baseline_summ_type = "single-ext"
                    k = 3
                # Compare models. Use question-driven models here
                datasets = {
                    '{k}-random sentences'.format(k=k): "data/baselines/chiqa_eval/baseline_random_sentences_k_{k}_chiqa_{l}-{t}.json".format(l=baseline_summ_task, t=baseline_summ_type, k=k),
                    '{k}-first sentences'.format(k=k): "data/baselines/chiqa_eval/baseline_first_sentences_k_{k}_chiqa_{l}-{t}.json".format(l=baseline_summ_task, t=baseline_summ_type, k=k),
                    '{k}-best ROUGE'.format(k=k): "data/baselines/chiqa_eval/baseline_best_rouge_k_{k}_chiqa_{l}-{t}.json".format(l=baseline_summ_task, t=baseline_summ_type, k=k),
                    'SC-{k}'.format(k=k): "data/sentence_classifier/chiqa_eval/sent_class_chiqa_{l}_{t}_dropout_5_sent_200_tok_50_val_20_d_256_l2_reg_binary_topk{k}.json".format(l=summ_task, t=summ_type, k=k),
                    'BART': "data/bart/chiqa_eval/bart_chiqa_{q}_{l}_{t}.json".format(q=question_string, l=summ_task, t=summ_type),
                    'Pointer Generator': "data/pointer_generator/chiqa_eval/pointergen_chiqa_bioasq_abs2summ_{q}_{l}_{t}.json".format(q=question_string, l=summ_task, t=summ_type),
                }

                processor = DataProcessor()
                data = {}
                for dataset in datasets:
                   data[dataset] = processor.load_data(datasets[dataset])

                # Use these to make pandas latex table
                experiments = []
                rouge_l_list = []
                rouge_w_list = []
                rouge_1_list = []
                rouge_2_list = []
                bleu_list = []

                for eval_set in data:
                    questions = data[eval_set][2]
                    # Save rouge2 per sample to excel
                    if summ_type == "single_extractive" and summ_task == "page2answer" and args.rouge_per_sample:
                        rouge_scores = []
                        tokenized_gold = tokenize(data[eval_set][0])
                        tokenized_guess = tokenize(data[eval_set][1])
                        for gs, gld, question in zip(tokenized_guess, tokenized_gold, questions):
                            rouge_scores.append(compute_fast_rouge([gs], [gld]))
                        ann_dict = {'Gold summs': data[eval_set][0], 'Gen summ': data[eval_set][1], 'ROUGE-2': rouge_scores}
                        df = pd.DataFrame(ann_dict)
                        df.to_excel("results/{e}_{l}_{t}_rouge2-per-summ.xlsx".format(e=eval_set, l=summ_task, t=summ_type), index=False)

                    if summ_type == "single_extractive" and summ_task == "page2answer" and args.bleu_per_sample:
                        bleu_scores = []
                        for gs, gld, question in zip(data[eval_set][1], data[eval_set][0], questions):
                            bleu_scores.append(nltk.translate.bleu_score.corpus_bleu([[gld]], [gs]))
                        ann_dict = {'Gold summs': data[eval_set][0], 'Gen summ': data[eval_set][1], 'BLEU': bleu_scores}
                        df = pd.DataFrame(ann_dict)
                        df.to_excel("results/{e}_{l}_{t}_bleu-per-summ.xlsx".format(e=eval_set, l=summ_task, t=summ_type), index=False)

                    experiments.append(eval_set)
                    rouge_1, rouge_2, rouge_l, rouge_w = calculate_rouge(eval_set, data[eval_set], summ_task, summ_type)
                    rouge_1_list.append(rouge_1)
                    rouge_2_list.append(rouge_2)
                    rouge_l_list.append(rouge_l)

                    if args.calculate_wilcoxon:
                        if "BART" in eval_set or "Pointer" in eval_set:
                            rouge_scores = []
                            tokenized_gold = tokenize(data[eval_set][0])
                            tokenized_guess = tokenize(data[eval_set][1])
                            for gs, gld in zip(tokenized_guess, tokenized_gold):
                                rouge_scores.append(compute_fast_rouge([gs], [gld]))
                            if "{l}_{t}".format(l=summ_task, t=summ_type) in rouge_dict:
                                rouge_dict["{l}_{t}".format(l=summ_task, t=summ_type)][eval_set] = rouge_scores
                            else:
                                rouge_dict["{l}_{t}".format(l=summ_task, t=summ_type)] = {}
                                rouge_dict["{l}_{t}".format(l=summ_task, t=summ_type)][eval_set] = rouge_scores
                    # Use nltk to calculate bleu
                    if args.calculate_bleu:
                        bleu = compute_nltk_bleu(eval_set, data[eval_set])
                        bleu_list.append(bleu)

                table_dict = {
                    'Experiment': experiments,
                    'ROUGE-1': rouge_1_list,
                    'ROUGE-2': rouge_2_list,
                    'ROUGE-L': rouge_l_list,
                    }
                if args.calculate_bleu:
                    table_dict['BLEU'] = bleu_list

                results_table = pd.DataFrame(table_dict)
                results_table.to_latex("results/asumm_table_chiqa_{l}_{t}.tex".format(l=summ_task, t=summ_type), index=False)

        results_file.close()
        if args.calculate_wilcoxon:
            for exp in rouge_dict:
                print(exp)
                p_value = scipy.stats.wilcoxon(rouge_dict[exp]['BART'], rouge_dict[exp]['Pointer Generator'])
                print(p_value)


    #focus on the question summ task with BART
    if args.dataset == "chiqa" and args.evaluate_question_driven:
        # Wilcoxon for BART with and without Q
        if args.calculate_wilcoxon:
            rouge_dict = {}
        for summ_task in ['page2answer', 'section2answer']:
            experiments = []
            rouge_l_list = []
            rouge_1_list = []
            rouge_2_list = []
            bleu_list = []
            for summ_type in ['single_abstractive', 'single_extractive']:
                for q in ["with_question", "without_question"]:
                    print("evaluating: {q}, {l}, {t}".format(q=q, l=summ_task, t=summ_type))
                    datasets = {
                        'BART-{q}-{t}'.format(q=q, t=summ_type): "data/bart/chiqa_eval/bart_chiqa_{q}_{l}_{t}.json".format(q=q, l=summ_task, t=summ_type),
                    }

                    processor = DataProcessor()
                    data = {}
                    for dataset in datasets:
                       data[dataset] = processor.load_data(datasets[dataset])

                    for eval_set in data:
                        questions = data[eval_set][2]
                        # Save rouge2 per sample to excel for a task/model/summ type
                        if summ_type == "single_abstractive" and summ_task == "page2answer" and "BART" in eval_set:
                            if args.rouge_per_sample:
                                rouge_scores = []
                                tokenized_gold = tokenize(data[eval_set][0])
                                tokenized_guess = tokenize(data[eval_set][1])
                                for gs, gld, question in zip(tokenized_guess, tokenized_gold, questions):
                                    rouge_scores.append(compute_fast_rouge([gs], [gld]))
                                ann_dict = {'Questions': questions, 'Gold summs': data[eval_set][0], 'Gen summ {}'.format(q): data[eval_set][1], 'ROUGE-2': rouge_scores}
                                df = pd.DataFrame(ann_dict)
                                df.to_excel("results/{e}_{l}_rouge2-per-summ.xlsx".format(e=eval_set, l=summ_task), index=False)

                        # Compute wilcoxon between two models: With question and without question, for all BART experiments
                        if args.calculate_wilcoxon:
                            if "BART" in eval_set:
                                rouge_scores = []
                                tokenized_gold = tokenize(data[eval_set][0])
                                tokenized_guess = tokenize(data[eval_set][1])
                                for gs, gld in zip(tokenized_guess, tokenized_gold):
                                    rouge_scores.append(compute_fast_rouge([gs], [gld]))
                                if "{l}_{t}".format(l=summ_task, t=summ_type) in rouge_dict:
                                    rouge_dict["{l}_{t}".format(l=summ_task, t=summ_type)][q] = rouge_scores
                                else:
                                    rouge_dict["{l}_{t}".format(l=summ_task, t=summ_type)] = {}
                                    rouge_dict["{l}_{t}".format(l=summ_task, t=summ_type)][q] = rouge_scores

                        experiments.append(eval_set)
                        rouge_1, rouge_2, rouge_l, rouge_w = calculate_rouge(eval_set, data[eval_set], summ_task, summ_type)
                        rouge_1_list.append(rouge_1)
                        rouge_2_list.append(rouge_2)
                        rouge_l_list.append(rouge_l)

                        # Use nltk to calculate bleu
                        if args.calculate_bleu:
                            bleu = compute_nltk_bleu(eval_set, data[eval_set])
                            bleu_list.append(bleu)

            # Save table for section task and page task
            table_dict = {
                'Experiment': experiments,
                'ROUGE-1': rouge_1_list,
                'ROUGE-2': rouge_2_list,
                'ROUGE-L': rouge_l_list,
                }
            if args.calculate_bleu:
                table_dict['BLEU'] = bleu_list
            results_table = pd.DataFrame(table_dict)
            results_table.to_latex("results/chiqa_question_driven_{l}.tex".format(l=summ_task), index=False)

        if args.calculate_wilcoxon:
            for exp in rouge_dict:
                print(exp)
                p_value = scipy.stats.wilcoxon(rouge_dict[exp]['with_question'], rouge_dict[exp]['without_question'])
                print(p_value)


if __name__ == "__main__":
    global results_file
    global args
    args = get_args().parse_args()
    results_file = open("results/{}_evaluation.txt".format(args.dataset), "w", encoding="utf-8")
    evaluate()
